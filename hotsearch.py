#!/usr/bin/env python3
"""
çƒ­æœæ•°æ®è·å–æµ‹è¯•
- UAPI: å¾®åšã€ç™¾åº¦ã€çŸ¥ä¹ã€Bç«™ã€æŠ–éŸ³ã€ä»Šæ—¥å¤´æ¡
- TianAPI: å¾®ä¿¡çƒ­æœ
- ITAPI: å°çº¢ä¹¦çƒ­ç‚¹
- æ™ºèƒ½ç­›é€‰ï¼šè·¨å¹³å°çƒ­åº¦èšåˆï¼Œé€‰å‡ºæœ€é‡è¦çš„10æ¡
- é£ä¹¦æ¨é€ï¼šå‘é€åˆ°é£ä¹¦ç¾¤
"""

import json
import urllib.request
import urllib.error
import urllib.parse
import ssl
import re
import os
from datetime import datetime
from collections import defaultdict
from pathlib import Path

# é£ä¹¦ç¾¤ ID
FEISHU_GROUP_ID = "oc_3e108939f68467ddd73cedfb796642e8"

SSL_CONTEXT = ssl.create_default_context()
SSL_CONTEXT.check_hostname = False
SSL_CONTEXT.verify_mode = ssl.CERT_NONE

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "application/json, text/plain, */*",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
}

def load_env():
    env_file = Path(__file__).parent / ".env"
    if env_file.exists():
        with open(env_file, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#") and "=" in line:
                    key, value = line.split("=", 1)
                    os.environ.setdefault(key.strip(), value.strip())

load_env()

PROXY = os.environ.get("RSS_PROXY", "http://127.0.0.1:7890")

UAPI_BASE = "https://uapis.cn/api/v1/misc/hotboard"
TIANAPI_KEY = os.environ.get("TIANAPI_KEY", "")
TIANAPI_WXHOT = f"https://apis.tianapi.com/wxhottopic/index?key={TIANAPI_KEY}"
ITAPI_KEY = os.environ.get("ITAPI_KEY", "")
ITAPI_XIAOHONGSHU = f"https://api.itapi.cn/api/hotnews/xiaohongshu?key={ITAPI_KEY}"

UAPI_PLATFORMS = {
    "weibo": {"name": "å¾®åšçƒ­æœ", "source": "å¾®åš", "weight": 1.0},
    "baidu": {"name": "ç™¾åº¦çƒ­æœ", "source": "ç™¾åº¦", "weight": 1.0},
    "zhihu": {"name": "çŸ¥ä¹çƒ­æ¦œ", "source": "çŸ¥ä¹", "weight": 0.9},
    "bilibili": {"name": "Bç«™çƒ­æ¦œ", "source": "Bç«™", "weight": 0.8},
    "douyin": {"name": "æŠ–éŸ³çƒ­ç‚¹", "source": "æŠ–éŸ³", "weight": 1.0},
    "toutiao": {"name": "ä»Šæ—¥å¤´æ¡", "source": "ä»Šæ—¥å¤´æ¡", "weight": 0.9},
}

TIANAPI_PLATFORMS = {
    "weixin": {"name": "å¾®ä¿¡çƒ­æœ", "source": "å¾®ä¿¡", "weight": 1.2},
}

ITAPI_PLATFORMS = {
    "xiaohongshu": {"name": "å°çº¢ä¹¦çƒ­ç‚¹", "source": "å°çº¢ä¹¦", "weight": 0.9},
}

PLATFORM_ORDER = ["weibo", "baidu", "zhihu", "bilibili", "douyin", "toutiao", "weixin", "xiaohongshu"]

STOP_WORDS = {"çš„", "äº†", "æ˜¯", "åœ¨", "æœ‰", "å’Œ", "ä¸", "æˆ–", "ç­‰", "è¿™", "é‚£", "æˆ‘", "ä½ ", "ä»–", "å¥¹", "å®ƒ", "ä»¬", "ç€", "è¿‡", "è¢«", "æŠŠ", "ç»™", "å‘", "ä»", "åˆ°", "ä¸º", "ä»¥", "åŠ", "å…¶", "ä¹‹", "ä¸Š", "ä¸‹", "ä¸­", "å†…", "å¤–", "å‰", "å", "å·¦", "å³", "ä¸€", "äºŒ", "ä¸‰", "å››", "äº”", "å…­", "ä¸ƒ", "å…«", "ä¹", "å", "ç™¾", "åƒ", "ä¸‡", "äº¿", "ä¸ª", "åª", "æ¡", "ä»¶", "æ¬¡", "å", "ä½", "ç§", "ç±»", "æ ·", "äº›", "å¤š", "å°‘", "å¤§", "å°", "é•¿", "çŸ­", "é«˜", "ä½", "å¿«", "æ…¢", "æ–°", "è€", "å¥½", "å", "å¯¹", "é”™", "çœŸ", "å‡", "èƒ½", "ä¼š", "è¦", "å¯", "åº”", "è¯¥", "é¡»", "å¿…", "éœ€", "å°†", "å·²", "æ­£", "å†", "ä¹Ÿ", "å°±", "æ‰", "éƒ½", "åˆ", "è¿˜", "æ›´", "æœ€", "å¾ˆ", "å¤ª", "çœŸ", "å®", "é™…", "ç°", "å½“", "åº”", "è¯¥", "å› ", "æ‰€", "è€Œ", "ä½†", "å´", "åª", "ä»…", "å·²", "æ›¾", "å¸¸", "æ€»", "å…¨", "æ¯", "å„", "æŸ", "ä»»", "ä½•", "è°", "å“ª", "ä»€", "ä¹ˆ", "æ€", "æ ·", "å‡ ", "å¤š", "å°‘", "å¤š", "ä¹…", "è¿œ", "è¿‘", "è¿™", "é‚£", "æ­¤", "å½¼", "æŸ", "å„", "æ¯", "å‡¡", "è¯¸", "ä¼—", "ç¾¤", "äº›", "è‹¥", "å¦‚", "ä¼¼", "åƒ", "åŒ", "å¼‚", "æ¯”", "è¾ƒ", "æœ€", "æ›´", "å¾ˆ", "å¤ª", "æ", "ç”š", "é¢‡", "ç¨", "ç•¥", "è¾ƒ", "æ›´", "æœ€", "æ", "ç”š", "é¢‡", "ç¨", "ç•¥"}


def fetch_json(url: str, timeout: int = 15, headers: dict = None) -> dict:
    try:
        req = urllib.request.Request(url, headers=headers or HEADERS)
        handler = urllib.request.ProxyHandler({"http": PROXY, "https": PROXY})
        opener = urllib.request.build_opener(handler, urllib.request.HTTPSHandler(context=SSL_CONTEXT))
        with opener.open(req, timeout=timeout) as resp:
            return json.loads(resp.read().decode("utf-8"))
    except Exception as e:
        return {"error": str(e)}


def get_uapi_hot(platform: str, limit: int = 20) -> list:
    if platform not in UAPI_PLATFORMS:
        return []
    
    config = UAPI_PLATFORMS[platform]
    print(f"   æ–¹æ³•: UAPI ({config['name']})")
    
    url = f"{UAPI_BASE}?type={platform}"
    data = fetch_json(url)
    
    if "error" in data:
        print(f"   âŒ è¯·æ±‚å¤±è´¥: {data['error']}")
        return []
    
    if "list" not in data:
        print(f"   âŒ æ•°æ®æ ¼å¼é”™è¯¯")
        return []
    
    items = []
    for idx, item in enumerate(data["list"][:limit], 1):
        items.append({
            "title": item.get("title", ""),
            "hot": item.get("hot_value", ""),
            "url": item.get("url", ""),
            "source": config["source"],
            "platform": platform,
            "rank": idx,
            "weight": config["weight"],
        })
    
    print(f"   âœ… è·å– {len(items)} æ¡")
    return items


def get_weixin_hot(limit: int = 20) -> list:
    config = TIANAPI_PLATFORMS["weixin"]
    print(f"   æ–¹æ³•: TianAPI ({config['name']})")
    
    data = fetch_json(TIANAPI_WXHOT)
    
    if "error" in data:
        print(f"   âŒ è¯·æ±‚å¤±è´¥: {data['error']}")
        return []
    
    if data.get("code") != 200:
        print(f"   âŒ APIé”™è¯¯: {data.get('msg', 'æœªçŸ¥é”™è¯¯')}")
        return []
    
    items = []
    for idx, item in enumerate(data.get("result", {}).get("list", [])[:limit], 1):
        items.append({
            "title": item.get("word", ""),
            "hot": "",
            "url": f"https://weixin.sogou.com/weixin?type=2&query={urllib.parse.quote(item.get('word', ''))}",
            "source": config["source"],
            "platform": "weixin",
            "rank": idx,
            "weight": config["weight"],
        })
    
    print(f"   âœ… è·å– {len(items)} æ¡")
    return items


def get_xiaohongshu_hot(limit: int = 20) -> list:
    config = ITAPI_PLATFORMS["xiaohongshu"]
    print(f"   æ–¹æ³•: ITAPI ({config['name']})")
    
    data = fetch_json(ITAPI_XIAOHONGSHU)
    
    if "error" in data:
        print(f"   âŒ è¯·æ±‚å¤±è´¥: {data['error']}")
        return []
    
    if data.get("code") != 200:
        print(f"   âŒ APIé”™è¯¯: {data.get('msg', 'æœªçŸ¥é”™è¯¯')}")
        return []
    
    items = []
    for item in data.get("data", [])[:limit]:
        items.append({
            "title": item.get("name", ""),
            "hot": item.get("viewnum", ""),
            "url": item.get("url", ""),
            "source": config["source"],
            "platform": "xiaohongshu",
            "rank": item.get("rank", 0),
            "weight": config["weight"],
        })
    
    print(f"   âœ… è·å– {len(items)} æ¡")
    return items


def get_hot_list(platform: str, limit: int = 20) -> list:
    if platform == "weixin":
        return get_weixin_hot(limit)
    elif platform == "xiaohongshu":
        return get_xiaohongshu_hot(limit)
    elif platform in UAPI_PLATFORMS:
        return get_uapi_hot(platform, limit)
    else:
        print(f"   âŒ ä¸æ”¯æŒçš„å¹³å°: {platform}")
        return []


def get_all_hot_lists(platforms: list = None, limit: int = 20) -> dict:
    if platforms is None:
        platforms = PLATFORM_ORDER
    
    results = {}
    for platform in platforms:
        results[platform] = get_hot_list(platform, limit)
    
    return results


def get_platform_name(platform: str) -> str:
    if platform in UAPI_PLATFORMS:
        return UAPI_PLATFORMS[platform]["name"]
    elif platform in TIANAPI_PLATFORMS:
        return TIANAPI_PLATFORMS[platform]["name"]
    elif platform in ITAPI_PLATFORMS:
        return ITAPI_PLATFORMS[platform]["name"]
    return platform


def get_platform_weight(platform: str) -> float:
    if platform in UAPI_PLATFORMS:
        return UAPI_PLATFORMS[platform]["weight"]
    elif platform in TIANAPI_PLATFORMS:
        return TIANAPI_PLATFORMS[platform]["weight"]
    elif platform in ITAPI_PLATFORMS:
        return ITAPI_PLATFORMS[platform]["weight"]
    return 1.0


def normalize_title(title: str) -> str:
    title = re.sub(r'[^\w\s\u4e00-\u9fff]', '', title)
    title = title.lower().strip()
    return title


def extract_keywords(title: str) -> set:
    normalized = normalize_title(title)
    words = set()
    current_word = ""
    
    for char in normalized:
        if '\u4e00' <= char <= '\u9fff':
            if current_word:
                if current_word not in STOP_WORDS and len(current_word) > 1:
                    words.add(current_word)
                current_word = ""
            if char not in STOP_WORDS:
                words.add(char)
        elif char.isalnum():
            current_word += char
        else:
            if current_word and current_word not in STOP_WORDS and len(current_word) > 1:
                words.add(current_word)
            current_word = ""
    
    if current_word and current_word not in STOP_WORDS and len(current_word) > 1:
        words.add(current_word)
    
    return words


def jaccard_similarity(set1: set, set2: set) -> float:
    if not set1 or not set2:
        return 0.0
    intersection = len(set1 & set2)
    union = len(set1 | set2)
    return intersection / union if union > 0 else 0.0


def calculate_similarity(title1: str, title2: str) -> float:
    if not title1 or not title2:
        return 0.0
    
    norm1 = normalize_title(title1)
    norm2 = normalize_title(title2)
    
    if norm1 == norm2:
        return 1.0
    
    if norm1 in norm2 or norm2 in norm1:
        return 0.9
    
    kw1 = extract_keywords(title1)
    kw2 = extract_keywords(title2)
    
    return jaccard_similarity(kw1, kw2)


def parse_hot_value(hot_str: str) -> float:
    if not hot_str:
        return 0.0
    
    hot_str = str(hot_str).strip()
    
    try:
        if 'äº¿' in hot_str:
            num = float(re.sub(r'[^\d.]', '', hot_str.replace('äº¿', '')))
            return num * 100000000
        elif 'ä¸‡' in hot_str:
            num = float(re.sub(r'[^\d.]', '', hot_str.replace('ä¸‡', '')))
            return num * 10000
        else:
            num = float(re.sub(r'[^\d.]', '', hot_str))
            return num
    except:
        return 0.0


def select_top_news(all_items: list, top_n: int = 10, similarity_threshold: float = 0.6) -> list:
    if not all_items:
        return []
    
    clusters = []
    
    for item in all_items:
        matched = False
        for cluster in clusters:
            if calculate_similarity(item["title"], cluster["title"]) >= similarity_threshold:
                cluster["items"].append(item)
                matched = True
                break
        
        if not matched:
            clusters.append({
                "title": item["title"],
                "items": [item],
            })
    
    scored_clusters = []
    for cluster in clusters:
        items = cluster["items"]
        
        platform_count = len(set(item["platform"] for item in items))
        platform_score = platform_count * 100
        
        rank_scores = []
        for item in items:
            rank_score = (21 - item["rank"]) * item["weight"]
            rank_scores.append(rank_score)
        avg_rank_score = sum(rank_scores) / len(rank_scores) if rank_scores else 0
        
        hot_values = []
        for item in items:
            hot = parse_hot_value(item.get("hot", ""))
            if hot > 0:
                hot_values.append(hot)
        max_hot = max(hot_values) if hot_values else 0
        hot_score = min(max_hot / 100000, 50)
        
        total_score = platform_score + avg_rank_score + hot_score
        
        platforms = [item["source"] for item in items]
        best_item = max(items, key=lambda x: (parse_hot_value(x.get("hot", "")), -x["rank"]))
        
        scored_clusters.append({
            "title": best_item["title"],
            "url": best_item["url"],
            "hot": best_item.get("hot", ""),
            "platforms": platforms,
            "platform_count": platform_count,
            "score": total_score,
        })
    
    scored_clusters.sort(key=lambda x: (-x["platform_count"], -x["score"]))
    
    return scored_clusters[:top_n]


def format_top_news(top_items: list) -> str:
    if not top_items:
        return "æš‚æ— æ•°æ®"
    
    lines = []
    for i, item in enumerate(top_items, 1):
        title = item["title"]
        if len(title) > 30:
            title = title[:30] + "..."
        
        hot_str = f"ğŸ”¥{item['hot']}" if item.get("hot") else ""
        cross_str = f"[{item['platform_count']}å¹³å°]" if item["platform_count"] > 1 else ""
        
        info_parts = [p for p in [hot_str, cross_str] if p]
        info_str = " ".join(info_parts)
        
        lines.append(f"**{i}. {title}**")
        if info_str:
            lines.append(f"   {info_str}")
        lines.append(f"   [æŸ¥çœ‹è¯¦æƒ…]({item['url']})")
        lines.append("")
    
    return "\n".join(lines)


def format_hot_list(items: list, show_hot: bool = True) -> str:
    if not items:
        return "æš‚æ— æ•°æ®"
    
    lines = []
    for i, item in enumerate(items, 1):
        title = item["title"]
        if len(title) > 25:
            title = title[:25] + "..."
        
        hot_str = f"ğŸ”¥{item['hot']}" if show_hot and item.get("hot") else ""
        
        if hot_str:
            lines.append(f"{i}. {title} {hot_str}")
        else:
            lines.append(f"{i}. {title}")
    
    return "\n".join(lines)


def generate_markdown_report(results: dict, top_news: list) -> str:
    now = datetime.now().strftime("%Y-%m-%d %H:%M")
    
    lines = [
        "# ğŸ“± çƒ­æœé€Ÿé€’",
        "",
        f"â° {now}",
        "",
        "---",
        "",
        "## ğŸ”¥ TOP 10 çƒ­ç‚¹",
        "",
    ]
    
    lines.append(format_top_news(top_news))
    
    lines.append("---")
    lines.append("")
    lines.append("## ğŸ“‹ å„å¹³å°çƒ­æœ")
    lines.append("")
    
    for platform, items in results.items():
        name = get_platform_name(platform)
        
        lines.append(f"**{name}**")
        if items:
            lines.append(format_hot_list(items))
        else:
            lines.append("âŒ è·å–å¤±è´¥")
        lines.append("")
    
    return "\n".join(lines)


def format_feishu_message(top_items: list, total_count: int, platforms: list) -> str:
    """æ ¼å¼åŒ–é£ä¹¦æ¶ˆæ¯"""
    if not top_items:
        return "æš‚æ— æ•°æ®"

    lines = ["ğŸ“± ä»Šæ—¥çƒ­ç‚¹é€Ÿé€’", ""]
    lines.append(f"â° {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    lines.append("")
    lines.append("---")
    lines.append("")
    lines.append("ğŸ”¥ TOP 10 çƒ­ç‚¹")
    lines.append("")

    for i, item in enumerate(top_items, 1):
        title = item["title"]
        if len(title) > 30:
            title = title[:30] + "..."
        
        hot_str = f"ğŸ”¥{item['hot']}" if item.get("hot") else ""
        cross_str = f"[{item['platform_count']}å¹³å°]" if item["platform_count"] > 1 else ""
        
        info_parts = [p for p in [hot_str, cross_str] if p]
        info_str = " ".join(info_parts)
        
        lines.append(f"**{i}. {title}**")
        if info_str:
            lines.append(f"   {info_str}")
        lines.append(f"   [æŸ¥çœ‹è¯¦æƒ…]({item['url']})")
        lines.append("")

    lines.append("---")
    lines.append("")
    lines.append(f"ğŸ“Š ç»Ÿè®¡: {total_count}æ¡çƒ­æœ | {len(platforms)}ä¸ªå¹³å°")

    return "\n".join(lines)


def send_to_feishu(message: str) -> bool:
    """å‘é€æ¶ˆæ¯åˆ°é£ä¹¦ç¾¤"""
    try:
        import subprocess
        import os

        # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆnvm ç¯å¢ƒï¼‰
        env = os.environ.copy()
        env["PATH"] = f"/Users/linux/.nvm/versions/node/v22.22.0/bin:{env['PATH']}"

        # OpenClaw å‘½ä»¤è·¯å¾„
        openclaw_path = "/Users/linux/.nvm/versions/node/v22.22.0/bin/openclaw"

        cmd = [
            openclaw_path, "message", "send",
            "--channel", "feishu",
            "--target", FEISHU_GROUP_ID,
            "--message", message
        ]

        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30, env=env)
        if result.returncode == 0:
            print("âœ… å·²æˆåŠŸå‘é€åˆ°é£ä¹¦ç¾¤")
            return True
        else:
            print(f"âŒ å‘é€åˆ°é£ä¹¦å¤±è´¥: {result.stderr}")
            return False
    except Exception as e:
        print(f"âŒ å‘é€åˆ°é£ä¹¦å¤±è´¥: {e}")
        return False


def main():
    print("=" * 60)
    print("çƒ­æœæ•°æ®è·å–æµ‹è¯• - æ™ºèƒ½ç­›é€‰ç‰ˆ")
    print("=" * 60)
    
    results = get_all_hot_lists(PLATFORM_ORDER)
    
    all_items = []
    for platform, items in results.items():
        all_items.extend(items)
    
    print(f"\nï¿½ å…±è·å– {len(all_items)} æ¡çƒ­æœæ•°æ®")
    
    print("\nğŸ” æ™ºèƒ½ç­›é€‰ TOP 10...")
    top_news = select_top_news(all_items, top_n=10)
    
    print("\n" + "=" * 60)
    print("ğŸ”¥ ä»Šæ—¥çƒ­ç‚¹ TOP 10:")
    print("=" * 60)
    for i, item in enumerate(top_news, 1):
        platform_str = "ã€".join(item["platforms"][:3])
        if len(item["platforms"]) > 3:
            platform_str += f"ç­‰{len(item['platforms'])}å¹³å°"
        print(f"{i}. {item['title']}")
        print(f"   æ¥æº: {platform_str} | å¾—åˆ†: {item['score']:.1f}")
    
    print("\n" + "=" * 60)
    print("ç”Ÿæˆå®Œæ•´æŠ¥å‘Š...")
    
    report = generate_markdown_report(results, top_news)
    
    output_file = "/tmp/hotsearch-test.md"
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(report)
    
    print("\n" + "=" * 60)
    print(f"âœ… å·²ä¿å­˜åˆ°: {output_file}")

    # å‘é€åˆ°é£ä¹¦ç¾¤
    print("\n" + "=" * 60)
    print("ğŸ“¤ å‘é€åˆ°é£ä¹¦ç¾¤...")

    feishu_message = format_feishu_message(top_news, len(all_items), list(results.keys()))
    if send_to_feishu(feishu_message):
        print("âœ… å·²æˆåŠŸå‘é€åˆ°é£ä¹¦ç¾¤")
    else:
        print("âŒ å‘é€åˆ°é£ä¹¦ç¾¤å¤±è´¥")


if __name__ == "__main__":
    main()
